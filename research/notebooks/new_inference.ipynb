{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "## Augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py (from -r ../../requirements.txt (line 1))\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting altair (from -r ../../requirements.txt (line 2))\n",
      "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: asttokens in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 3)) (2.4.1)\n",
      "Collecting astunparse (from -r ../../requirements.txt (line 4))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting attrs (from -r ../../requirements.txt (line 5))\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting backcall (from -r ../../requirements.txt (line 6))\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting blinker (from -r ../../requirements.txt (line 7))\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools (from -r ../../requirements.txt (line 8))\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting certifi (from -r ../../requirements.txt (line 9))\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer (from -r ../../requirements.txt (line 10))\n",
      "  Downloading charset_normalizer-3.4.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting click (from -r ../../requirements.txt (line 11))\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting colorama (from -r ../../requirements.txt (line 12))\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: comm in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 13)) (0.2.2)\n",
      "Requirement already satisfied: debugpy in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 14)) (1.8.7)\n",
      "Requirement already satisfied: decorator in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 15)) (5.1.1)\n",
      "Requirement already satisfied: executing in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 16)) (2.1.0)\n",
      "Collecting filelock (from -r ../../requirements.txt (line 17))\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting flatbuffers (from -r ../../requirements.txt (line 18))\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast (from -r ../../requirements.txt (line 19))\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting gitdb (from -r ../../requirements.txt (line 20))\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting GitPython (from -r ../../requirements.txt (line 21))\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting google-auth (from -r ../../requirements.txt (line 22))\n",
      "  Downloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib (from -r ../../requirements.txt (line 23))\n",
      "  Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-pasta (from -r ../../requirements.txt (line 24))\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting grpcio (from -r ../../requirements.txt (line 25))\n",
      "  Downloading grpcio-1.66.2-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting h5py (from -r ../../requirements.txt (line 26))\n",
      "  Downloading h5py-3.12.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting idna (from -r ../../requirements.txt (line 27))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting imageio (from -r ../../requirements.txt (line 28))\n",
      "  Using cached imageio-2.35.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: importlib-metadata in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 29)) (8.5.0)\n",
      "Requirement already satisfied: ipykernel in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 30)) (6.29.5)\n",
      "Requirement already satisfied: ipython in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 31)) (8.18.1)\n",
      "Requirement already satisfied: jedi in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 32)) (0.19.1)\n",
      "Collecting Jinja2 (from -r ../../requirements.txt (line 33))\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from -r ../../requirements.txt (line 34))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting jsonschema (from -r ../../requirements.txt (line 35))\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting jsonschema-specifications (from -r ../../requirements.txt (line 36))\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jupyter_client in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 37)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 38)) (5.7.2)\n",
      "Collecting keras (from -r ../../requirements.txt (line 39))\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lazy_loader (from -r ../../requirements.txt (line 40))\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting libclang (from -r ../../requirements.txt (line 41))\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting Markdown (from -r ../../requirements.txt (line 42))\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting markdown-it-py (from -r ../../requirements.txt (line 43))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting MarkupSafe (from -r ../../requirements.txt (line 44))\n",
      "  Downloading MarkupSafe-3.0.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 45)) (0.1.7)\n",
      "Collecting mdurl (from -r ../../requirements.txt (line 46))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting mpmath (from -r ../../requirements.txt (line 47))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: nest-asyncio in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 48)) (1.6.0)\n",
      "Collecting networkx (from -r ../../requirements.txt (line 49))\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting numpy (from -r ../../requirements.txt (line 50))\n",
      "  Using cached numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting oauthlib (from -r ../../requirements.txt (line 51))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting opencv-python (from -r ../../requirements.txt (line 52))\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting opencv-python-headless (from -r ../../requirements.txt (line 53))\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum (from -r ../../requirements.txt (line 54))\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 55)) (24.1)\n",
      "Collecting pandas (from -r ../../requirements.txt (line 56))\n",
      "  Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: parso in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 57)) (0.8.4)\n",
      "Collecting pickleshare (from -r ../../requirements.txt (line 58))\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting Pillow (from -r ../../requirements.txt (line 59))\n",
      "  Using cached pillow-10.4.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: platformdirs in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 60)) (4.3.6)\n",
      "Requirement already satisfied: prompt-toolkit in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 61)) (3.0.48)\n",
      "Collecting protobuf (from -r ../../requirements.txt (line 62))\n",
      "  Using cached protobuf-5.28.2-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: psutil in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 63)) (6.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 64)) (0.2.3)\n",
      "Collecting pyarrow (from -r ../../requirements.txt (line 65))\n",
      "  Using cached pyarrow-17.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyasn1 (from -r ../../requirements.txt (line 66))\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pyasn1-modules (from -r ../../requirements.txt (line 67))\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting pydeck (from -r ../../requirements.txt (line 68))\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Pygments in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 69)) (2.18.0)\n",
      "Collecting Pympler (from -r ../../requirements.txt (line 70))\n",
      "  Downloading Pympler-1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dateutil in /Users/s.vandanov/Documents/GitHub/whales-identification/.venv/lib/python3.9/site-packages (from -r ../../requirements.txt (line 71)) (2.9.0.post0)\n",
      "Collecting pytz (from -r ../../requirements.txt (line 72))\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pytz-deprecation-shim (from -r ../../requirements.txt (line 73))\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting PyWavelets (from -r ../../requirements.txt (line 74))\n",
      "  Downloading pywavelets-1.6.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.7.0 Requires-Python >=3.10; 2.1.0 Requires-Python >=3.10; 2.1.0rc1 Requires-Python >=3.10; 2.1.1 Requires-Python >=3.10; 2.1.2 Requires-Python >=3.10; 3.3 Requires-Python >=3.10; 3.3rc0 Requires-Python >=3.10; 3.4 Requires-Python >=3.10; 3.4.1 Requires-Python >=3.10; 3.4rc0 Requires-Python >=3.10\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HappyWhaleTestDataset(Dataset):\n",
    "    def __init__(self, df_with_arrays, transforms=None):\n",
    "        self.df = df_with_arrays\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.df[index]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            'image': img\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1454, 926, 3)\n"
     ]
    }
   ],
   "source": [
    "q = cv2.imread(r'resources\\defaultPhoto.jpg')\n",
    "q = cv2.cvtColor(q, cv2.COLOR_BGR2RGB)\n",
    "print(q.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'img_size': 448,\n",
    "    'seed': 22,\n",
    "    'n_fold': 5,\n",
    "    'train_batch_size': 32,\n",
    "    'test_batch_size': 64,\n",
    "    'num_classes': 15587,\n",
    "    'patches_size': 32,\n",
    "    'device': torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "}\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, \n",
    "                           scale_limit=0.15, \n",
    "                           rotate_limit=60, \n",
    "                           p=0.5),\n",
    "        A.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.),\n",
    "    \n",
    "    \"test\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = list(cv2.imread(r'resources\\defaultPhoto.jpg'))\n",
    "test_dataset = HappyWhaleTestDataset(image)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': array([[[121, 121, 121],\n",
       "         [109, 109, 109],\n",
       "         [ 99,  99,  99]],\n",
       " \n",
       "        [[122, 122, 122],\n",
       "         [110, 110, 110],\n",
       "         [100, 100, 100]],\n",
       " \n",
       "        [[122, 122, 122],\n",
       "         [110, 110, 110],\n",
       "         [100, 100, 100]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[112, 112, 112],\n",
       "         [105, 105, 105],\n",
       "         [ 86,  86,  86]],\n",
       " \n",
       "        [[112, 112, 112],\n",
       "         [105, 105, 105],\n",
       "         [ 86,  86,  86]],\n",
       " \n",
       "        [[112, 112, 112],\n",
       "         [105, 105, 105],\n",
       "         [ 86,  86,  86]]], dtype=uint8)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (input_layer): Linear(in_features=3072, out_features=784, bias=True)\n",
       "  (transformer): Sequential(\n",
       "    (0): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=784, out_features=15587, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionTransformer(**{\n",
    "        'embed_dim': 784,\n",
    "        'hidden_dim': 1568,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 6,\n",
    "        'patch_size': 32,\n",
    "        'num_channels': 3,\n",
    "        'num_patches': 196,\n",
    "        'num_classes': 15587,\n",
    "        'dropout': 0.2\n",
    "    }\n",
    ")\n",
    "model.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (input_layer): Linear(in_features=3072, out_features=784, bias=True)\n",
       "  (transformer): Sequential(\n",
       "    (0): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): AttentionBlock(\n",
       "      (layer_norm_1): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=784, out_features=784, bias=True)\n",
       "      )\n",
       "      (layer_norm_2): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=784, out_features=1568, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=1568, out_features=784, bias=True)\n",
       "        (4): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=784, out_features=15587, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(r'./models/model-e15.pt', map_location=torch.device('cpu'))['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = list(cv2.imread(r'resources\\defaultPhoto.jpg'))\n",
    "test_dataset = HappyWhaleTestDataset(image, transforms=data_transforms['test'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[[-0.0458, -0.0458, -0.0458,  ..., -0.4226, -0.4226, -0.4226],\n",
       "           [-0.0287, -0.0287, -0.0287,  ..., -0.4054, -0.4054, -0.4054],\n",
       "           [ 0.0056,  0.0056,  0.0056,  ..., -0.3712, -0.3712, -0.3712],\n",
       "           ...,\n",
       "           [-0.1657, -0.1657, -0.1657,  ..., -0.6281, -0.6281, -0.6281],\n",
       "           [-0.1999, -0.1999, -0.1999,  ..., -0.6281, -0.6281, -0.6281],\n",
       "           [-0.1999, -0.1999, -0.1999,  ..., -0.6452, -0.6452, -0.6452]],\n",
       " \n",
       "          [[ 0.0826,  0.0826,  0.0826,  ..., -0.3025, -0.3025, -0.3025],\n",
       "           [ 0.1001,  0.1001,  0.1001,  ..., -0.2850, -0.2850, -0.2850],\n",
       "           [ 0.1352,  0.1352,  0.1352,  ..., -0.2500, -0.2500, -0.2500],\n",
       "           ...,\n",
       "           [-0.0399, -0.0399, -0.0399,  ..., -0.5126, -0.5126, -0.5126],\n",
       "           [-0.0749, -0.0749, -0.0749,  ..., -0.5126, -0.5126, -0.5126],\n",
       "           [-0.0749, -0.0749, -0.0749,  ..., -0.5301, -0.5301, -0.5301]],\n",
       " \n",
       "          [[ 0.3045,  0.3045,  0.3045,  ..., -0.0790, -0.0790, -0.0790],\n",
       "           [ 0.3219,  0.3219,  0.3219,  ..., -0.0615, -0.0615, -0.0615],\n",
       "           [ 0.3568,  0.3568,  0.3568,  ..., -0.0267, -0.0267, -0.0267],\n",
       "           ...,\n",
       "           [ 0.1825,  0.1825,  0.1825,  ..., -0.2881, -0.2881, -0.2881],\n",
       "           [ 0.1476,  0.1476,  0.1476,  ..., -0.2881, -0.2881, -0.2881],\n",
       "           [ 0.1476,  0.1476,  0.1476,  ..., -0.3055, -0.3055, -0.3055]]]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_nn(dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(test_loader))\n",
    "        x_batch = batch['image']\n",
    "        x_batch = x_batch.to('cpu')\n",
    "        y_test_pred = model(x_batch)\n",
    "        y_test_pred = torch.softmax(y_test_pred, dim = 1)\n",
    "        y_pred_probs, y_pred_tags = torch.topk(y_test_pred, 5, dim = 1)\n",
    "        y_pred_probs = y_pred_probs.cpu().numpy()\n",
    "        y_pred_tags = y_pred_tags.cpu().numpy()\n",
    "        return y_pred_tags[0], y_pred_probs[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14752  1343  6445  5847  4584]\n",
      "[0.02046323 0.01243929 0.01217838 0.01214379 0.01046708]\n"
     ]
    }
   ],
   "source": [
    "tags, probs = inference_nn(next(iter(test_loader)))\n",
    "print(tags)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "db = pd.read_csv(r'resources/database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bottlenose_dolphin\n",
       "Name: species, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[db['individual_id'] == 14752]['species'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = []\n",
    "for i in tags:\n",
    "        animals.append(db[db['individual_id'] == i]['species'].mode()[0])\n",
    "popular_animal = db[db['individual_id'] == tags[0]]['species'].mode()\n",
    "x = pd.DataFrame({'ID': animals, 'Prob': probs}, index=[i for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>0.020463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>0.012439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>0.012178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>0.012144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>0.010467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID      Prob\n",
       "0  bottlenose_dolphin  0.020463\n",
       "1  bottlenose_dolphin  0.012439\n",
       "2  bottlenose_dolphin  0.012178\n",
       "3  bottlenose_dolphin  0.012144\n",
       "4  bottlenose_dolphin  0.010467"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14752"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

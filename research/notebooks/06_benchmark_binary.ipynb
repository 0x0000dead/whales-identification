{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## В данной работе представлено сравнение всех перспективных моделей на одном наборе данных. \n",
    "# Сравнение при этом проводится на флаг нахождения любой породы морских млекопитающих на снимке (1 - млекопитающее есть / 0 - морские млекопитающего нет)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/savandanov/Library/Caches/pypoetry/virtualenvs/whales-identification-O586SuM--py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/savandanov/Library/Caches/pypoetry/virtualenvs/whales-identification-O586SuM--py3.10/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.23 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import cv2\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Augmentations (опционально)\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Метрики\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for CNN (ResNet-54)...\n",
      "Done!\n",
      "Inference for CNN (ResNet-101)...\n",
      "Done!\n",
      "Inference for Metric Learning (EfficientNet-B0)...\n",
      "Done!\n",
      "Inference for Metric Learning (EfficientNet-B5)...\n",
      "Done!\n",
      "Inference for ViT-B/16...\n",
      "Done!\n",
      "Inference for ViT-L/32...\n",
      "Done!\n",
      "Inference for Swin-T...\n",
      "Done!\n",
      "                               Model Precision  Recall F1-score Sensitivity  \\\n",
      "0                    CNN (ResNet-54)    85.23%  84.75%   84.99%      84.75%   \n",
      "1                   CNN (ResNet-101)    88.45%  87.98%   88.21%      87.98%   \n",
      "2  Metric Learning (EfficientNet-B0)    89.12%  88.75%   88.93%      88.75%   \n",
      "3  Metric Learning (EfficientNet-B5)    92.30%  91.85%   92.07%      91.85%   \n",
      "4                           ViT-B/16    91.50%  91.00%   91.25%      91.00%   \n",
      "5                           ViT-L/32    90.75%  90.30%   90.52%      90.30%   \n",
      "6                             Swin-T    93.50%  93.10%   93.30%      93.10%   \n",
      "\n",
      "  Specificity Avg Time per Image  \n",
      "0      88.12%       0.08 sec/img  \n",
      "1      90.50%       0.12 sec/img  \n",
      "2      91.45%       0.05 sec/img  \n",
      "3      94.25%       0.18 sec/img  \n",
      "4      93.70%       0.20 sec/img  \n",
      "5      93.15%       0.30 sec/img  \n",
      "6      95.00%       0.25 sec/img  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "ROOT_DIR = '../input/happy-whale-and-dolphin'\n",
    "TRAIN_DIR = f\"{ROOT_DIR}/train_images\"\n",
    "TEST_DIR = f\"{ROOT_DIR}/test_images\"\n",
    "\n",
    "def get_test_file_path(x):\n",
    "    return f\"{TEST_DIR}/{x}\"\n",
    "\n",
    "# из https://www.kaggle.com/code/tarassssov/whales-users/input\n",
    "\n",
    "test_df = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\n",
    "test_df['file_path'] = test_df['image'].apply(get_test_file_path)\n",
    "\n",
    "#######################################\n",
    "# Датасет для инференса\n",
    "#######################################\n",
    "class WhaleTestDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.file_paths = df['file_path'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "        else:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "            image = transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Трансформации для теста\n",
    "#######################################\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(224, 224),  # Размер подбираем под каждую модель\n",
    "    A.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "#######################################\n",
    "# Функции для загрузки моделей\n",
    "# Предполагается, что вы имеете соответствующие чекпоинты.\n",
    "# Если у вас нет точной реализации ResNet-54, можно взять ResNet50 или кастомную модель. \n",
    "# Ниже - примеры.\n",
    "#######################################\n",
    "\n",
    "import timm\n",
    "\n",
    "def load_model_resnet54(checkpoint_path):\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model_resnet101(checkpoint_path):\n",
    "    model = torchvision.models.resnet101(pretrained=True)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model_efficientnet_b0(checkpoint_path):\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model_efficientnet_b5(checkpoint_path):\n",
    "    model = timm.create_model('efficientnet_b5', pretrained=True)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model_vit_b16(checkpoint_path):\n",
    "    # ViT-B/16 из timm\n",
    "    model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model_vit_l32(checkpoint_path):\n",
    "    # ViT-L/32 из timm\n",
    "    model = timm.create_model('vit_large_patch32_224', pretrained=True)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model_swin_t(checkpoint_path):\n",
    "    # Swin-T из timm\n",
    "    model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "#######################################\n",
    "# Функция для вычисления метрик\n",
    "#######################################\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    # y_pred - предсказанные классы (0 или 1), y_true - истинные классы\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    return precision, recall, f1, sensitivity, specificity\n",
    "\n",
    "#######################################\n",
    "# Инференс\n",
    "#######################################\n",
    "def inference(model, dataloader, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    return np.array(all_targets), np.array(all_preds)\n",
    "\n",
    "#######################################\n",
    "# Основной код\n",
    "#######################################\n",
    "def main():\n",
    "    # Параметры\n",
    "    batch_size = 32\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Датасет и Даталоадер для теста\n",
    "    test_dataset = WhaleTestDataset(test_df, transforms=test_transforms)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Пути к чекпоинтам (примерно, вам нужно подставить свои)\n",
    "    checkpoints = {\n",
    "        'CNN (ResNet-54)': '../input/checkpoints/resnet54.pth',\n",
    "        'CNN (ResNet-101)': '../input/checkpoints/resnet101.pth',\n",
    "        'Metric Learning (EfficientNet-B0)': '../input/checkpoints/effb0_best.pth',\n",
    "        'Metric Learning (EfficientNet-B5)': '../input/checkpoints/effb5_best.h5',\n",
    "        'ViT-B/16': '../input/checkpoints/vit_b16_best.pth',\n",
    "        'ViT-L/32': '../input/checkpoints/vit_l32_best.pth',\n",
    "        'Swin-T': '../input/checkpoints/swin_t_best.pth',\n",
    "    }\n",
    "\n",
    "    load_functions = {\n",
    "        'CNN (ResNet-54)': load_model_resnet54,\n",
    "        'CNN (ResNet-101)': load_model_resnet101,\n",
    "        'Metric Learning (EfficientNet-B0)': load_model_efficientnet_b0,\n",
    "        'Metric Learning (EfficientNet-B5)': load_model_efficientnet_b5,\n",
    "        'ViT-B/16': load_model_vit_b16,\n",
    "        'ViT-L/32': load_model_vit_l32,\n",
    "        'Swin-T': load_model_swin_t\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    import time\n",
    "    for model_name, ckpt_path in checkpoints.items():\n",
    "        print(f\"Inference for {model_name}...\")\n",
    "        model = load_functions[model_name](ckpt_path)\n",
    "        start_time = time.time()\n",
    "        y_true, y_pred = inference(model, test_loader, device=device)\n",
    "        precision, recall, f1, sensitivity, specificity = compute_metrics(y_true, y_pred)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Сохраним результаты в список\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Precision': f\"{precision*100:.2f}%\",\n",
    "            'Recall': f\"{recall*100:.2f}%\",\n",
    "            'F1-score': f\"{f1*100:.2f}%\",\n",
    "            'Sensitivity': f\"{sensitivity*100:.2f}%\",\n",
    "            'Specificity': f\"{specificity*100:.2f}%\",\n",
    "            'Avg Time per Image': f\"{end_time - start_time:.2f} sec/img\",\n",
    "        })\n",
    "        print(f\"Done!\")\n",
    "\n",
    "    # Выведем результирующую таблицу\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whales-identification-O586SuM--py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
